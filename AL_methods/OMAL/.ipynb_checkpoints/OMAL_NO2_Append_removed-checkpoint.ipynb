{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0355f92-7104-4030-8e8c-2aebd9837375",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from modAL.models import ActiveLearner, CommitteeRegressor\n",
    "from modAL.disagreement import vote_entropy_sampling, max_std_sampling\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.nn import MSELoss\n",
    "from skorch.regressor import NeuralNetRegressor\n",
    "from copy import deepcopy\n",
    "from skorch.dataset import ValidSplit\n",
    "from skorch.callbacks import EarlyStopping\n",
    "from sklearn.cluster import DBSCAN, OPTICS, cluster_optics_dbscan, Birch, SpectralClustering\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn import metrics\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn import Linear\n",
    "from torch.nn import Sigmoid, ReLU\n",
    "from torch.nn import Module\n",
    "from sklearn.metrics import r2_score\n",
    "import copy\n",
    "from scipy.spatial import distance\n",
    "from scipy.spatial.distance import euclidean\n",
    "from torch import tensor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from model import MLP\n",
    "from model_state import get_model_params, get_input_for_hidden_layers, get_model_params_gradientNorm\n",
    "from functions_batch2 import get_variance, qbc, normalization, error_reduct_fuc, total_disagrement, grad_norm, averaged_grad_norm, get_avg_grad_norm, training_loss\n",
    "import random\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler, Freezer, Unfreezer\n",
    "from torch.optim.lr_scheduler import CyclicLR, ReduceLROnPlateau\n",
    "from skorch.regressor import NeuralNetRegressor, NeuralNet\n",
    "from model_state import features_concat, learning_state_features_concat\n",
    "import gc\n",
    "import warnings\n",
    "from modAL.utils.selection import multi_argmax\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dceaa95-f840-4923-9958-b246f5d1e771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-params:\n",
    "query_number = 30          # The number of the AL Iterations in each Exp\n",
    "iteration = 20             # Total number of the Exps\n",
    "batch_size = 10\n",
    "total_initail_size = 12\n",
    "initial_size = 5\n",
    "dimensions = 7   # Total dimension of the data features + model states\n",
    "\n",
    "seed_rf = np.load(file=\"..\\..\\seed2.npy\")\n",
    "seed_initial = np.load(file=\"..\\..\\seed3.npy\")\n",
    "\n",
    "seed_nn1 = np.load(file=\"..\\..\\seed4.npy\")\n",
    "seed_nn2 = np.load(file=\"..\\..\\seed4.npy\")\n",
    "seed_nn3 = np.load(file=\"..\\..\\seed5.npy\")\n",
    "seed_nn4 = np.load(file=\"..\\..\\seed5.npy\")\n",
    "seed_nn5 = np.load(file=\"..\\..\\seed6.npy\")\n",
    "seed_nn6 = np.load(file=\"..\\..\\seed6.npy\")\n",
    "\n",
    "seed_predictor = np.load(file=\"..\\..\\seed7.npy\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3664799-9b42-448e-94de-f424b15c7a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Committee_Prediction(learner_list, X_test, y_test):\n",
    "    initial_pred=[]\n",
    "    for i in learner_list:\n",
    "        initial_pred.append(r2_score(y_test, i.predict(X_test)))\n",
    "    initial_pred=np.array(initial_pred)\n",
    "    \n",
    "    return initial_pred.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0488cd6c-903f-4969-88eb-404a307cfbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_function(query_number, iters):\n",
    "    # For saving results:\n",
    "    rf_model_training_r2 = []\n",
    "    rf_model_training_mse = []\n",
    "    rf_model_testing_r2 = []\n",
    "    rf_model_testing_mse = []\n",
    "    \n",
    "    # Number of member in committee:\n",
    "    n_members = 3\n",
    "    learner_list = []\n",
    "    \n",
    "    # The model for evaluation:\n",
    "    rf_model = RandomForestRegressor(random_state=seed_rf[iters], n_estimators=100)\n",
    "    gradient_norms = np.empty(shape=(0, 0))\n",
    "    \n",
    "    # Load the data:\n",
    "    name1 = \"..\\..\\Datasets\\\\NO2\\X_train\" + str(iters) + \".npy\"\n",
    "    name2 = \"..\\..\\Datasets\\\\NO2\\X_test\" + str(iters) + \".npy\"\n",
    "    name3 = \"..\\..\\Datasets\\\\NO2\\y_train\" + str(iters) + \".npy\"\n",
    "    name4 = \"..\\..\\Datasets\\\\NO2\\y_test\" + str(iters) + \".npy\"\n",
    "    \n",
    "    X_train = np.load(name1, allow_pickle=True).astype(np.float32)\n",
    "    X_test = np.load(name2, allow_pickle=True).astype(np.float32)\n",
    "    y_train = np.load(name3, allow_pickle=True).astype(np.float32).reshape(-1, 1)\n",
    "    y_test = np.load(name4, allow_pickle=True).astype(np.float32).reshape(-1, 1)\n",
    "    \n",
    "    # Dimensionality of data:\n",
    "    X = X_train.shape[1]\n",
    "    # The index of the unlabeled pool:\n",
    "    X_index = np.arange(X_train.shape[0])\n",
    "\n",
    "    used_data = np.empty(shape=(0, X))\n",
    "    used_label = np.empty(shape=(0)).reshape(-1, 1)\n",
    "\n",
    "    X_initial = np.empty(shape=(0,X))\n",
    "    y_initial = np.empty(shape=(0)).reshape(-1, 1)\n",
    "    \n",
    "    # Initial Stage 1:\n",
    "    np.random.seed(seed_initial[iters])\n",
    "    idx = np.random.choice(range(len(X_index)), size=initial_size, replace=False)\n",
    "    train_idx = X_index[idx]\n",
    "\n",
    "    X_initial = X_train[train_idx]\n",
    "    y_initial = y_train[train_idx].reshape(-1, 1)\n",
    "\n",
    "    used_data = np.append(used_data, X_initial, axis=0).astype(np.float32)\n",
    "    used_label = np.append(used_label, y_initial, axis=0).astype(np.float32).reshape(-1, 1)\n",
    "    X_index = np.delete(X_index, idx, axis=0)\n",
    "\n",
    "    \n",
    "    used_data = torch.from_numpy(used_data).to(device)\n",
    "    used_label = torch.from_numpy(used_label).to(device)\n",
    "    for member_idx in range(n_members):\n",
    "        if member_idx == 0:\n",
    "            np.random.seed(seed_nn1[iters])\n",
    "            torch.manual_seed(seed_nn2[iters])\n",
    "        if member_idx == 1:\n",
    "            np.random.seed(seed_nn3[iters])\n",
    "            torch.manual_seed(seed_nn4[iters])\n",
    "        if member_idx == 2:\n",
    "            np.random.seed(seed_nn5[iters])\n",
    "            torch.manual_seed(seed_nn6[iters])\n",
    "            \n",
    "        regressor = NeuralNetRegressor(MLP(X),\n",
    "                                   criterion=MSELoss(),\n",
    "                                   optimizer=torch.optim.Adam,\n",
    "                                   verbose=0,\n",
    "                                   max_epochs=30,\n",
    "                                   lr=0.001,\n",
    "                                   # Used for the batch AL\n",
    "                                   callbacks=[EarlyStopping(patience=5), ('lr_scheduler', LRScheduler(policy=ReduceLROnPlateau))],\n",
    "                                   train_split=ValidSplit(cv=5),\n",
    "                                   warm_start=True,\n",
    "                                   device='cuda',\n",
    "                                   batch_size = 200\n",
    "                                   )\n",
    "        regressor.fit(used_data, used_label)\n",
    "        learner_list.append(regressor)\n",
    "    \n",
    "    print(\"NN Test R2 with 5 samples\", Committee_Prediction(learner_list, X_test, y_test))\n",
    "\n",
    "    # Number of LAL features:\n",
    "    learning_state_features = np.empty(shape=(0, dimensions))\n",
    "    loss_reduction_target = np.empty(shape=(0)).reshape(-1, 1)\n",
    "\n",
    "    # other random samples:\n",
    "    rest_initial_X = np.empty(shape=(0, X))\n",
    "    rest_initial_y = np.empty(shape=(0)).reshape(-1, 1)\n",
    "\n",
    "    # LAL Initialization: Stage 2\n",
    "    predictor = RandomForestRegressor(n_estimators=1000, random_state=seed_predictor[iters])\n",
    "    np.random.seed(seed_initial[iters])\n",
    "    idx = np.random.choice(range(len(X_index)), size=total_initail_size-initial_size, replace=False)\n",
    "    train_idx = X_index[idx]\n",
    "    \n",
    "    rest_initial_X = np.append(rest_initial_X, X_train[train_idx], axis=0).astype(np.float32)\n",
    "    rest_initial_y = np.append(rest_initial_y, y_train[train_idx], axis=0).astype(np.float32).reshape(-1, 1)\n",
    "\n",
    "    X_index = np.delete(X_index, idx, axis=0)\n",
    "\n",
    "    for i in range(rest_initial_X.shape[0]):\n",
    "\n",
    "        single_X = rest_initial_X[i],\n",
    "        single_y = rest_initial_y[i].reshape(1, -1)\n",
    "        single_X = single_X[0].reshape(1, -1)\n",
    "        \n",
    "        single_X = torch.from_numpy(single_X).to(device)\n",
    "        single_y = torch.from_numpy(single_y).to(device)\n",
    "\n",
    "        # Model params\n",
    "        model_params = get_model_params_gradientNorm(learner_list)\n",
    "\n",
    "        # updated data\n",
    "        used_data = torch.cat((used_data, single_X), 0)\n",
    "        used_label = torch.cat((used_label, single_y), 0)\n",
    "\n",
    "        # Retrain the model:\n",
    "        for l in learner_list:\n",
    "            l.fit(X=used_data,y=used_label)\n",
    "            \n",
    "        single_X = single_X.cpu().numpy()\n",
    "        single_y = single_y.cpu().numpy()\n",
    "            \n",
    "        index = train_idx[i]\n",
    "\n",
    "        # The training sample:\n",
    "        data_params = X_train[index]\n",
    "      \n",
    "        LALfeatures = data_params # features_concat(model_params, data_params)\n",
    "\n",
    "        loss_reduction = get_avg_grad_norm(learner_list, single_X, single_y)\n",
    "        \n",
    "        learning_state_features = learning_state_features_concat(learning_state_features, LALfeatures)\n",
    "        loss_reduction_target = np.append(loss_reduction_target, np.array([loss_reduction]).reshape(-1, 1), axis=0).astype(np.float32).reshape(-1, 1)\n",
    "\n",
    "        \n",
    "    used_data = used_data.cpu().numpy()\n",
    "    used_label = used_label.cpu().numpy()\n",
    "\n",
    "    # Finished the Initialization Stages:\n",
    "    # RF Regressor for evaluation:\n",
    "    rf_model.fit(used_data, used_label.ravel())\n",
    "    # Training Scores:\n",
    "    rf_training_r2 = r2_score(used_label, rf_model.predict(used_data))\n",
    "    rf_training_mse = mean_squared_error(used_label, rf_model.predict(used_data))\n",
    "    rf_model_training_r2.append(rf_training_r2)\n",
    "    rf_model_training_mse.append(rf_training_mse)\n",
    "    \n",
    "    # Test Scores:\n",
    "    rf_model_r2 = r2_score(y_test, rf_model.predict(X_test))\n",
    "    rf_model_mse = mean_squared_error(y_test, rf_model.predict(X_test))\n",
    "    rf_model_testing_r2.append(rf_model_r2)\n",
    "    rf_model_testing_mse.append(rf_model_mse)\n",
    "    \n",
    "    print(\"After Initialization RF R2:\", rf_model_r2)\n",
    "    print(\"NN Test R2 after Initialization\", Committee_Prediction(learner_list, X_test, y_test))\n",
    "    \n",
    "    print(np.unique(used_data, axis=0).shape)\n",
    "    for idx in range(query_number):\n",
    "        np.random.seed(None)\n",
    "        print('Query no. %d' % (idx+1))\n",
    "        # Reuse training data:\n",
    "        learning_state_features, loss_reduction_target = reuse_labeled_data_meta(used_data, used_label, learner_list, learning_state_features, loss_reduction_target)\n",
    "\n",
    "        print(learning_state_features.shape)\n",
    "\n",
    "        Error_reduction_list, training_score = error_reduct_fuc(X_train, X_index, learner_list, learning_state_features, loss_reduction_target, X_index.shape[0], predictor)\n",
    "    \n",
    "        idx = multi_argmax(np.array(Error_reduction_list), n_instances=batch_size)\n",
    "       \n",
    "        X_train_indices = X_index[idx]\n",
    "        \n",
    "        # Update\n",
    "        X_index = np.delete(X_index, idx, axis=0)\n",
    "\n",
    "        for X_train_index in X_train_indices:\n",
    "\n",
    "            new_X = X_train[X_train_index].reshape(1, -1)\n",
    "            new_y = y_train[X_train_index].reshape(1, -1)\n",
    "            \n",
    "            used_data = np.append(used_data, new_X, axis=0).astype(np.float32)\n",
    "            used_label = np.append(used_label, new_y, axis=0).astype(np.float32).reshape(-1, 1)\n",
    "            \n",
    "            used_data = torch.from_numpy(used_data).to(device)\n",
    "            used_label = torch.from_numpy(used_label).to(device)\n",
    "\n",
    "            # Model params\n",
    "            model_params = get_model_params_gradientNorm(learner_list)\n",
    "            data_params = X_train[X_train_index]\n",
    "            LALfeatures = data_params # features_concat(model_params, data_params)\n",
    "            \n",
    "            for l in learner_list:\n",
    "                l.fit(used_data, used_label)\n",
    "            \n",
    "            used_data = used_data.cpu().numpy()\n",
    "            used_label = used_label.cpu().numpy()\n",
    "\n",
    "            loss_reduction = get_avg_grad_norm(learner_list, new_X, new_y)  \n",
    "            gradient_norms = np.append(gradient_norms, loss_reduction)\n",
    "            \n",
    "            learning_state_features = learning_state_features_concat(learning_state_features, LALfeatures)\n",
    "            loss_reduction_target = np.append(loss_reduction_target, np.array([loss_reduction]).reshape(-1, 1), axis=0).astype(np.float32).reshape(-1, 1)\n",
    "            \n",
    "        # RF Evaluation\n",
    "        rf_model.fit(used_data, used_label.ravel())\n",
    "        \n",
    "        # Training Evaluation:\n",
    "        rf_training_r2 = r2_score(used_label, rf_model.predict(used_data))\n",
    "        rf_training_mse = mean_squared_error(used_label, rf_model.predict(used_data))\n",
    "        rf_model_training_r2.append(rf_training_r2)\n",
    "        rf_model_training_mse.append(rf_training_mse)\n",
    "        \n",
    "        # Test Evaluation:\n",
    "        rf_model_r2 = r2_score(y_test, rf_model.predict(X_test))\n",
    "        rf_model_mse = mean_squared_error(y_test, rf_model.predict(X_test))\n",
    "        rf_model_testing_r2.append(rf_model_r2)\n",
    "        rf_model_testing_mse.append(rf_model_mse)\n",
    "\n",
    "        print(np.unique(used_data, axis=0).shape)\n",
    "        print(\"RF R2:\", rf_model_r2)\n",
    "        print(\"Remaining:\", X_index.shape[0])\n",
    "       \n",
    "        # print('NN R2', Committee_Prediction(learner_list, X_test, y_test))\n",
    "\n",
    "    # RF\n",
    "    rf_model_testing_r2 = np.array(rf_model_testing_r2)\n",
    "    rf_model_testing_mse = np.array(rf_model_testing_mse)\n",
    "    rf_model_training_r2 = np.array(rf_model_training_r2)\n",
    "    rf_model_training_mse = np.array(rf_model_training_mse)\n",
    "    \n",
    "    # Used_data and Unsed_label:\n",
    "    np.save(file=\"..\\..\\Results\\Res_NO2\\OMAL_append_removed\\Summary\\\\used_data\" + str(iters) + \".npy\", arr=used_data)\n",
    "    np.save(file=\"..\\..\\Results\\Res_NO2\\OMAL_append_removed\\Summary\\\\used_labels\" + str(iters) + \".npy\", arr=used_label)\n",
    "\n",
    "    np.save(file=\"..\\..\\Results\\Res_NO2\\OMAL_append_removed\\Summary\\\\testing_rf_r2_\" + str(iters) + \".npy\", arr=rf_model_testing_r2)\n",
    "    np.save(file=\"..\\..\\Results\\Res_NO2\\OMAL_append_removed\\Summary\\\\testing_rf_mse_\" + str(iters) + \".npy\", arr=rf_model_testing_mse)\n",
    "    np.save(file=\"..\\..\\Results\\Res_NO2\\OMAL_append_removed\\Summary\\\\training_rf_r2_\" + str(iters) + \".npy\", arr=rf_model_training_r2)\n",
    "    np.save(file=\"..\\..\\Results\\Res_NO2\\OMAL_append_removed\\Summary\\\\training_rf_mse_\" + str(iters) + \".npy\", arr=rf_model_training_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "189d514a-ed65-4608-83dd-a3e45d0aea3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reuse_labeled_data_meta(used_data, used_label, learner_list, learning_state_features, loss_reduction_target):\n",
    "    for i in range(used_data.shape[0]):\n",
    "            \n",
    "            new_X = used_data[i].reshape(1, -1)\n",
    "            new_y = used_label[i].reshape(1, -1)\n",
    "            \n",
    "            used_data_ = np.append(used_data, new_X, axis=0).astype(np.float32)\n",
    "            used_label_ = np.append(used_label, new_y, axis=0).astype(np.float32).reshape(-1, 1)\n",
    "            \n",
    "            used_data_ = torch.from_numpy(used_data_).to(device)\n",
    "            used_label_ = torch.from_numpy(used_label_).to(device)\n",
    "\n",
    "            # Model params\n",
    "            model_params = get_model_params_gradientNorm(learner_list)\n",
    "            data_params = new_X.reshape(-1,)\n",
    "            LALfeatures = data_params # features_concat(model_params, data_params)\n",
    "            \n",
    "            for l in learner_list:\n",
    "                l.fit(used_data_, used_label_)\n",
    "\n",
    "            loss_reduction = get_avg_grad_norm(learner_list, new_X, new_y)  \n",
    "            \n",
    "            learning_state_features = learning_state_features_concat(learning_state_features, LALfeatures)\n",
    "            loss_reduction_target = np.append(loss_reduction_target, np.array([loss_reduction]).reshape(-1, 1), axis=0).astype(np.float32).reshape(-1, 1)\n",
    "    \n",
    "    return learning_state_features, loss_reduction_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfb3f10b-dc78-4f89-97cb-8e635c25e5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Iteration is  0\n",
      "NN Test R2 with 5 samples -15.32991684106078\n",
      "After Initialization RF R2: 0.07907253029892569\n",
      "NN Test R2 after Initialization -2.5110978712737366\n",
      "(12, 7)\n",
      "Query no. 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 7 and the array at index 1 has size 15",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18932\\3927749472.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The Iteration is \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mmain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery_number\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18932\\4089235373.py\u001b[0m in \u001b[0;36mmain_function\u001b[1;34m(query_number, iters)\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Query no. %d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;31m# Reuse training data:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m         \u001b[0mlearning_state_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_reduction_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreuse_labeled_data_meta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mused_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mused_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearner_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_state_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_reduction_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_state_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18932\\2704018377.py\u001b[0m in \u001b[0;36mreuse_labeled_data_meta\u001b[1;34m(used_data, used_label, learner_list, learning_state_features, loss_reduction_target)\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mloss_reduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_avg_grad_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearner_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[0mlearning_state_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearning_state_features_concat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_state_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLALfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m             \u001b[0mloss_reduction_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_reduction_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss_reduction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\Projects\\OMAL_Project2\\AL_methods\\OMAL\\model_state.py\u001b[0m in \u001b[0;36mlearning_state_features_concat\u001b[1;34m(d1, d2)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlearning_state_features_concat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Applications\\anaconda3\\lib\\site-packages\\numpy\\core\\overrides.py\u001b[0m in \u001b[0;36mvstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mD:\\Applications\\anaconda3\\lib\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m         \u001b[0marrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 282\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Applications\\anaconda3\\lib\\site-packages\\numpy\\core\\overrides.py\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 7 and the array at index 1 has size 15"
     ]
    }
   ],
   "source": [
    "for i in range(iteration):\n",
    "    print(\"The Iteration is \", i)\n",
    "    main_function(query_number, i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
